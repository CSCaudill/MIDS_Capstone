{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#INSTALL PACKAGES IF NOT ALREADY INSTALLED (UNCOMMENT)\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install Requests\n",
    "# ! pip install html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial link: web scraping: http://first-web-scraper.readthedocs.io/en/latest/#act-3-web-scraping\n",
    "See notes at this link.\n",
    "\n",
    "Other useful links: http://www.ontheissues.org/2016_Senate_Web.htm --> Quotes to Senate candidate websites\n",
    "Votematch: http://www.ontheissues.org/Quiz/results2016.asp?results.x=0&results.y=0&quiz=SenateTOP&state=TOP&gstate=TOP&q1=1&q2=3&q3=1&q4=5&q5=3&q6=2&q7=3&q8=4&q9=2&q10=3&q11=2&q12=4&q13=2&q14=2&q15=2&q16=3&q17=1&q18=4&q19=4&q20=2&ZipCode=01719 \n",
    "Get all Candidates with results on edge of this page: http://www.ontheissues.org/Senate/Ron_Crumpton.htm OR http://www.ontheissues.org/Senate/Ron_Crumpton_AE.htm (first one likely better)\n",
    "https://pandas.pydata.org/pandas-docs/stable/10min.html -> Pandas quick tutorial\n",
    "https://pandas.pydata.org/pandas-docs/stable/tutorials.html -> Pandas deeper tutorial\n",
    "http://www.ontheissues.org/Senate/Senate115.htm -> Senate 115 example\n",
    "http://www.ontheissues.org/2014_AK_Senate.htm -> From Senate 115 link (though could just replace with list of years and states to pull candidates that ran)... can potentially pull ?all? prior year candidates from here... look into\n",
    "\n",
    "NOTE: are questions different for SenateMatch each of the years? Probably...\n",
    "NOTE: write which packages used for scraping (beyond beautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB SCRAPING: ONTHEISSUES\n",
    "##### EXAMPLE USED: CORY BOOKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Idea:\n",
    " - Iterate through, whenever at anchor, store anchor name\n",
    " - If is for a target table, load table into memory\n",
    " - Once hit next anchor, done\n",
    " - Save info from target table, move onto the next "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Using saved text file (of website) while don't have access to Wifi to be able to scrape in the meantime. Uncomment actual website when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT STEPS:\n",
    " - Load data into Pandas data frame\n",
    " - Write to csv\n",
    " - See if can pull state information from ontheissues\n",
    " - Get list of all senators that ran\n",
    " - Save as text file or csv\n",
    " - Run info creator in a loop to get into the same csv file\n",
    " - Match up with other State info and congressional result info into another data frame\n",
    " - Will likely take a custom mapping of candidate -> congressional result info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mississippi': 'MS', 'Northern Mariana Islands': 'MP', 'Oklahoma': 'OK', 'Wyoming': 'WY', 'Minnesota': 'MN', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arkansas': 'AR', 'New Mexico': 'NM', 'Indiana': 'IN', 'Maryland': 'MD', 'Louisiana': 'LA', 'Texas': 'TX', 'Tennessee': 'TN', 'Iowa': 'IA', 'Wisconsin': 'WI', 'Arizona': 'AZ', 'Michigan': 'MI', 'Kansas': 'KS', 'Utah': 'UT', 'Virginia': 'VA', 'Oregon': 'OR', 'Connecticut': 'CT', 'District of Columbia': 'DC', 'New Hampshire': 'NH', 'Idaho': 'ID', 'West Virginia': 'WV', 'South Carolina': 'SC', 'California': 'CA', 'Massachusetts': 'MA', 'Vermont': 'VT', 'Georgia': 'GA', 'North Dakota': 'ND', 'Pennsylvania': 'PA', 'Puerto Rico': 'PR', 'Florida': 'FL', 'Hawaii': 'HI', 'Kentucky': 'KY', 'Rhode Island': 'RI', 'Nebraska': 'NE', 'Missouri': 'MO', 'Ohio': 'OH', 'Alabama': 'AL', 'Illinois': 'IL', 'Virgin Islands': 'VI', 'South Dakota': 'SD', 'Colorado': 'CO', 'New Jersey': 'NJ', 'National': 'NA', 'Washington': 'WA', 'North Carolina': 'NC', 'Maine': 'ME', 'New York': 'NY', 'Montana': 'MT', 'Nevada': 'NV', 'Delaware': 'DE', 'Guam': 'GU'}\n"
     ]
    }
   ],
   "source": [
    "### STATES MAPPING NECESSARY FOR CONVERSION\n",
    "\n",
    "states_map = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "states_reverse_map = dict([(value, key) for (key, value) in states_map.iteritems()])\n",
    "print states_reverse_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPER 1: CANDIDATE NAMES (2016 SENATE RACES)\n",
    "#### USE ONLY ONE SCRAPER AT ONCE TO WRITE TO CSV\n",
    "#### Use the following cell as url_list, df1 (etc.) if looking to write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d85b1ad4e463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mBeautifulSoup\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhtml2text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kowen\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kowen\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\io\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_html\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_sql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_sql_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_sql_query\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msasreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_sas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_stata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kowen\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kowen\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mDatabaseError\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### LOAD WEBSITE ###\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "import html2text\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "### GET URL, WRITE TO FILE ###\n",
    "#~~UNCOMMENT TO ACTUALLY WRITE~~#\n",
    "url = 'http://www.ontheissues.org/Senate/Ron_Crumpton.htm'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "page = urllib2.urlopen(url)\n",
    "html_content = page.read()\n",
    "rendered_content = html2text.html2text(html_content)\n",
    "\n",
    "#write to text file to make scrapable\n",
    "with open('text_files/curr_page.txt', 'w') as f :\n",
    "    f.write(rendered_content)\n",
    "f.close()\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "with open('text_files/curr_page.txt') as f :\n",
    "    text = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# print text\n",
    "\n",
    "#print website for reference\n",
    "# print text\n",
    "\n",
    "###### SCRAPE VOTEMATCH TABLE ######\n",
    "#NOTE: \"a names\" demarcate the topics in the votematch table\n",
    "\n",
    "#set initial lists, dict to be able to append to later\n",
    "candidate_list = []\n",
    "url_switch = False\n",
    "curr_state = None\n",
    "\n",
    "#iterate through each line in the web page (text) until get to portion with Candidate Names and urls\n",
    "for line in text :\n",
    "    \n",
    "    #flip switch if get to relevant line\n",
    "    if \"2016_AK_Senate.htm\" in line :\n",
    "        url_switch = True\n",
    "    \n",
    "    if url_switch is True :\n",
    "        #crossed out or \"old\" candidates not appearing in election have \"~~\"\n",
    "        if \"~~\" in line :\n",
    "            continue\n",
    "        \n",
    "        #take out current State\n",
    "        if 'Senate.htm' in line :\n",
    "            curr_state = line[1:3]\n",
    "\n",
    "        #print lines if want to troubleshoot\n",
    "#         print line            \n",
    "            \n",
    "        #get name as well as urls associated\n",
    "        if \"/Senate/\" in line :\n",
    "            full_name = line[line.find(\"/Senate/\")+8:line.find(\".htm)(\")]\n",
    "            party = line[line.find(\".htm)(\")+6:line.find(\".htm)(\")+7]\n",
    "            first_name, last_name = full_name.split(\"_\",1)\n",
    "            url_name = 'http://www.ontheissues.org/Senate/'+full_name+'.htm'\n",
    "            #single fix for J_L__Mealer\n",
    "            if full_name == \"J_L__Mealer\" :\n",
    "                first_name = \"J_L\"\n",
    "                last_name = \"Mealer\"\n",
    "            candidate_list.append((full_name,first_name,last_name,party,curr_state,url_name))\n",
    "            \n",
    "    if \"[Senate incumbents](../Senate.htm)\" in line :\n",
    "        url_switch = False\n",
    "\n",
    "#create initial dataframe\n",
    "df1 = pd.DataFrame(candidate_list)\n",
    "df1.columns = (\"full_name\",\"first_name\",\"last_name\",\"party\",\"state\",\"url\")\n",
    "\n",
    "#create names_list, url_list for reference in next section\n",
    "urls_list = list(df1[\"url\"])\n",
    "names_list = list(df1[\"full_name\"])\n",
    "url_to_name = [(url, name) for (url, name) in zip (urls_list, names_list)]\n",
    "url_to_name = dict(url_to_name)\n",
    "\n",
    "display(df1.head())\n",
    "print urls_list\n",
    "print len(urls_list)\n",
    "print len(set(df1[\"state\"]))\n",
    "print url_to_name\n",
    "\n",
    "write_name = 'text_files/2016_candidates_ontheissues_info.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPER 2: SENATE 115 (2016) SENATOR INFO\n",
    "#### USE ONLY ONE SCRAPER AT ONCE TO WRITE TO CSV\n",
    "#### Can easily change to SENATE 114, 113, etc, if needed\n",
    "#### Use the following cell as url_list, df1 (etc.) if looking to write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### LOAD WEBSITE ###\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "import html2text\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "### GET URL, WRITE TO FILE ###\n",
    "#~~UNCOMMENT TO ACTUALLY WRITE~~#\n",
    "url = 'http://www.ontheissues.org/Senate/Senate115.htm'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "page = urllib2.urlopen(url)\n",
    "html_content = page.read()\n",
    "rendered_content = html2text.html2text(html_content)\n",
    "\n",
    "#write to text file to make scrapable\n",
    "with open('text_files/curr_page.txt', 'w') as f :\n",
    "    f.write(rendered_content)\n",
    "f.close()\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "with open('text_files/curr_page.txt') as f :\n",
    "    text = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#print text if want to see page\n",
    "# print text\n",
    "\n",
    "candidate_list = []\n",
    "states_toggle = False\n",
    "curr_state = None\n",
    "\n",
    "#get list of names; later, will convert to urls to scrape\n",
    "for line in text :\n",
    "    \n",
    "    #start picking up states\n",
    "    if \"Alabama** |\" in line :\n",
    "        states_toggle = True\n",
    "    \n",
    "    #pick up state\n",
    "    if \"** |\" in line and states_toggle is True :\n",
    "        curr_state = line[:line.find(\"**\")]\n",
    "        #modify to abbrev\n",
    "        curr_state = states_reverse_map[curr_state]\n",
    "        \n",
    "    #scrape line of info if Jr Senator or Sr Senator in line\n",
    "    if \"Jr Senator\" in line or \"Sr Senator\" in line :\n",
    "        \n",
    "        # get party\n",
    "        if \"Democrat\" in line :\n",
    "            party = \"D\"\n",
    "        elif \"Republican\" in line :\n",
    "            party = \"R\"\n",
    "        elif \"Independent\" in line :\n",
    "            party = \"I\"\n",
    "        elif \"Libertarian\" in line :\n",
    "            party = \"L\"\n",
    "        elif \"Green\" in line :\n",
    "            party = \"G\"\n",
    "        else :\n",
    "            party = \"O\"\n",
    "        \n",
    "        #get full name\n",
    "        #deal with more of the inconsistencies within ontheissues.com, then scrape the name\n",
    "        if (\"]\") in line :\n",
    "            full_name = line[1:line.find(\"]\")]\n",
    "        else :\n",
    "            full_name = line[1:line.find(\"|\")]\n",
    "            \n",
    "        first_name, last_name = full_name.split(\" \",1)\n",
    "        \n",
    "        #Make in consistent underscore format\n",
    "        full_name = full_name.replace(\" \",\"_\")\n",
    "        \n",
    "        #handle exceptions to get to proper url\n",
    "        full_name = full_name.replace(\"-\",\"_\")\n",
    "        if full_name == \"Joe_Manchin\" :\n",
    "            full_name = \"Joe_Manchin_III\"\n",
    "        \n",
    "        url_name = 'http://www.ontheissues.org/Senate/'+full_name+'.htm'            \n",
    "        \n",
    "        candidate_list.append((full_name,first_name,last_name,party,curr_state,url_name))\n",
    "                \n",
    "#create initial dataframe\n",
    "df1 = pd.DataFrame(candidate_list)\n",
    "df1.columns = (\"full_name\",\"first_name\",\"last_name\",\"party\",\"state\",\"url\")\n",
    "\n",
    "#create names_list, url_list for reference in next section\n",
    "urls_list = list(df1[\"url\"])\n",
    "names_list = list(df1[\"full_name\"])\n",
    "url_to_name = [(url, name) for (url, name) in zip (urls_list, names_list)]\n",
    "url_to_name = dict(url_to_name)\n",
    "\n",
    "display(df1.head())\n",
    "# print urls_list\n",
    "# print len(urls_list)\n",
    "# print len(set(df1[\"state\"]))\n",
    "# print url_to_name\n",
    "\n",
    "#SPECIFY UNIQUE WRITE NAME\n",
    "write_name = 'text_files/2016_senators_ontheissues_info.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET INDIVIDUAL CANDIDATE INFO FROM URLS_LIST PROVIDED, WRITE KEY INFO FROM WHICHEVER CELL RUN ABOVE INTO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### LOAD WEBSITE ###\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "\n",
    "## GET URL, WRITE TO FILE ###\n",
    "\n",
    "#set up to add to all data\n",
    "all_data = []    \n",
    "    \n",
    "candidate_counter = 0\n",
    "#for each candidate, iterate through, and get key candidate information about them.\n",
    "for url in urls_list :\n",
    "\n",
    "    ### - UNCOMMENT TO CONNECT TO EACH WEBSITE, WRITE TO TEXT FILE (IF HAVEN'T ALREADY) - ###\n",
    "#     response = requests.get(url)\n",
    "#     html = response.content\n",
    "#     soup = BeautifulSoup(html)\n",
    "#     with open('text_files/'+url_to_name[url]+'.txt', 'w') as f :\n",
    "#         f.writelines(html)\n",
    "#     f.close()\n",
    "    ### -------------------------------------------------------------------------------- ###\n",
    "    \n",
    "    with open('text_files/'+url_to_name[url]+'.txt') as f :\n",
    "        text = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    #print website for reference\n",
    "    # print text\n",
    "\n",
    "    ###### SCRAPE VOTEMATCH TABLE ######\n",
    "    #NOTE: \"a names\" demarcate the topics in the votematch table\n",
    "\n",
    "    #set counters to be able to add key stats to\n",
    "    counter = 0\n",
    "    lines = []\n",
    "    points = []\n",
    "    text_dict = {}\n",
    "    points_dict = {}\n",
    "    topics_dict = {}\n",
    "\n",
    "    #should reset as default dictionaries that then get oppose / favor for each sub-topic?\n",
    "    ## can set up \"GENERAL\" high-level and then \"IN-DEPTH\" if people want to get there, as well\n",
    "\n",
    "    #iterate through each line in the web page (text) until get to the votematch table\n",
    "    for line in text :\n",
    "        #once at votematch table, get to each section via \"a name\" divider\n",
    "        if \"a name\" in line or \"End Senatematch Responses\" in line:\n",
    "            #if not first time have hit \"a name\" section, reset, and add lines and points to dictionaries\n",
    "            if counter > 0 :\n",
    "                text_dict[counter] = lines\n",
    "                points_dict[counter] = points\n",
    "                topics_dict[counter] = (topic, category)\n",
    "                if \"End Senatematch Responses\" in line :\n",
    "                    #end of section, break for loop\n",
    "                    break\n",
    "\n",
    "            counter += 1\n",
    "            soup = BeautifulSoup(line, \"lxml\")\n",
    "            question_num = counter\n",
    "            lines = []\n",
    "\n",
    "        #if have hit \"a name\" section, then get number of points associated for that section\n",
    "        if counter > 0 :\n",
    "            lines.append(line)\n",
    "            if \"points on\" in line :\n",
    "                #grab number of points\n",
    "                points = line[line.find(\"(\")+1:line.find(\"points\")-1]\n",
    "                points = int(points)\n",
    "                #grab social vs economic category\n",
    "                if \"Social\" in line :\n",
    "                    category = \"Social\"\n",
    "                else :\n",
    "                    category = \"Economic\"\n",
    "            if \"</b> topic\" in line :\n",
    "                #grab topic mapping (only really necessary once...)\n",
    "                topic = line[line.find(\"'>\")+2:line.find(\"</a\")]\n",
    "\n",
    "\n",
    "    ### CREATE QUESTION -> TOPIC MAPPING FOR EACH GENERAL TOPIC ###\n",
    "    topics_list = []\n",
    "    for question, (topic, category) in topics_dict.iteritems() :\n",
    "        topics_list.append((question, topic, category))\n",
    "\n",
    "    ### CREATE QUESTION -> POINTS MAPPING FOR EACH GENERAL TOPIC ###\n",
    "    points_list = []\n",
    "    for question, points in points_dict.iteritems() :\n",
    "        points_list.append((question, points))\n",
    "\n",
    "    \n",
    "    ### GET TOTAL POINTS FOR SOCIAL, ECONOMIC ###\n",
    "\n",
    "    #map category (Social, Economic) to points_list_category\n",
    "    #may want to clean up with a standard \"map\"\n",
    "    points_category_list = []\n",
    "    social = 0\n",
    "    economic = 0\n",
    "    for (question, points) in points_list :\n",
    "        category = topics_dict[question][1]\n",
    "        points_category_list.append((question, points, category))\n",
    "        #add points to relevant category\n",
    "        if category == \"Social\" :\n",
    "            social += points\n",
    "        else :\n",
    "            economic += points\n",
    "            \n",
    "    #GET ALL KEY CURRENT DATA ON THE CANDIDATE FOR MODEL, APPEND TO ALL DATA\n",
    "    curr_data = [p[1] for p in points_list]\n",
    "    curr_data.append(social)\n",
    "    curr_data.append(economic)\n",
    "    curr_data.insert(0,url)\n",
    "    #WILL ALSO NEED TO APPEND URL_NAME\n",
    "    all_data.append(curr_data)\n",
    "\n",
    "    \n",
    "    #PRINT SECTION\n",
    "#     print \"POINTS DICT:\"\n",
    "#     for i in points_dict.iteritems() :\n",
    "#         print i\n",
    "\n",
    "#     print\n",
    "#     print \"TEXT DICT SECTION BREAKOUT\"\n",
    "#     for i in text_dict.iteritems() :\n",
    "#         print i\n",
    "#         print \n",
    "\n",
    "#     print \"TOPICS LIST:\"\n",
    "#     print topics_list\n",
    "\n",
    "#     print \"POINTS LIST:\"\n",
    "#     print points_list\n",
    "\n",
    "#     print \"CURR DATA LIST:\"\n",
    "#     print curr_data_list\n",
    "    \n",
    "#     print \"POINTS LIST WITH CATEGORY:\"\n",
    "#     print points_category_list\n",
    "#     print \n",
    "#     print \"SOCIAL POINTS:\"\n",
    "#     print social\n",
    "#     print \"ECONOMIC POINTS:\"\n",
    "#     print economic\n",
    "\n",
    "#exit outside of loop and print all data\n",
    "print all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and combine new data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DATA FRAME WITH ALL CANDIDATE SCORING ###\n",
    "df2 = pd.DataFrame(all_data)\n",
    "df2.columns = [\"url\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n",
    "               \"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"social\",\"economic\"]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DATA FRAME EXPLAINING EACH TOPIC NUMBER ###\n",
    "df3 = pd.DataFrame(topics_list)\n",
    "df3.columns = [\"topic_num\", \"topic_desc\", \"topic_cat\"]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MERGE TO CREATE CANDIDATE INFO\n",
    "df4 = pd.merge(df1,\n",
    "                 df2,\n",
    "                 on='url',\n",
    "              how='left')\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### FOR SOCIAL CATEGORIES, MULTIPLY BY -1 TO MAKE SAME \"DIRECTION\" FOR SOCIAL VS ECONOMIC FOR LEFT vs RIGHT WING\n",
    "def reverse_sign(num) :\n",
    "    return -1*num\n",
    "\n",
    "social_columns = [\"1\",\"3\",\"4\",\"8\",\"9\",\"12\",\"15\",\"16\",\"17\",\"19\",\"social\"]\n",
    "\n",
    "df4[social_columns]= df4[social_columns].apply(reverse_sign)\n",
    "\n",
    "display(df4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE AS CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4.to_csv(write_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3.to_csv('text_files/issue_number_mapper.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
