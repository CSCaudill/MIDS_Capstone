{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#INSTALL PACKAGES IF NOT ALREADY INSTALLED (UNCOMMENT)\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial link: web scraping: http://first-web-scraper.readthedocs.io/en/latest/#act-3-web-scraping\n",
    "See notes at this link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB SCRAPING: ONTHEISSUES\n",
    "##### EXAMPLE USED: CORY BOOKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Idea:\n",
    " - Iterate through, whenever at anchor, store anchor name\n",
    " - If is for a target table, load table into memory\n",
    " - Once hit next anchor, done\n",
    " - Save info from target table, move onto the next "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Using saved text file (of website) while don't have access to Wifi to be able to scrape in the meantime. Uncomment actual website when ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  2\n",
       "2  3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "y = pd.DataFrame(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT STEPS:\n",
    " - Load data into Pandas data frame\n",
    " - Write to csv\n",
    " - See if can pull state information from ontheissues\n",
    " - Get list of all senators that ran\n",
    " - Save as text file or csv\n",
    " - Run info creator in a loop to get into the same csv file\n",
    " - Match up with other State info and congressional result info into another data frame\n",
    " - Will likely take a custom mapping of candidate -> congressional result info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "writelines() takes exactly one argument (23 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-263e7b3caab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text_files/candidate_info.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_main\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#write titles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mf_main\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CANDIDATE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SOCIAL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ECONOMIC'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: writelines() takes exactly one argument (23 given)"
     ]
    }
   ],
   "source": [
    "### LOAD WEBSITE ###\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "\n",
    "### GET URL, WRITE TO FILE ###\n",
    "#~~UNCOMMENT TO ACTUALLY WRITE~~#\n",
    "# url = 'http://www.ontheissues.org/Cory_Booker.htm'\n",
    "# response = requests.get(url)\n",
    "# html = response.content\n",
    "# soup = BeautifulSoup(html)\n",
    "# with open('text_files/cory_booker.txt', 'w') as f :\n",
    "#     f.writelines(soup)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "\n",
    "candidates_list = ['cory_booker']\n",
    "\n",
    "### NOT WORKING: WANT TO RE-WRITE INTO A PANDAS TABLE FIRST THEN WRITE INTO CSV WITH OTHER INFO\n",
    "with open('text_files/candidate_info.txt','w') as f_main :\n",
    "    #write titles\n",
    "    f_main.writelines('CANDIDATE', 'SOCIAL', 'ECONOMIC', 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)\n",
    "    \n",
    "\n",
    "candidate_counter = 0\n",
    "#for each candidate, iterate through, and get key candidate information about them.\n",
    "for candidate in candidates_list :\n",
    "\n",
    "    #Hacky way of getting to data in table without internet\n",
    "    ### ~~~~~~~~ FIX UP TO URL ONCE HAVE INTERNET CONNECTION!!! ~~~~~~ ###\n",
    "    with open('text_files/'+candidate+'.txt') as f :\n",
    "        text = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    #print website for reference\n",
    "    # print text\n",
    "\n",
    "    ###### SCRAPE VOTEMATCH TABLE ######\n",
    "    #NOTE: \"a names\" demarcate the topics in the votematch table\n",
    "\n",
    "    #set counters to be able to add key stats to\n",
    "    counter = 0\n",
    "    lines = []\n",
    "    points = []\n",
    "    text_dict = {}\n",
    "    points_dict = {}\n",
    "    topics_dict = {}\n",
    "\n",
    "    #should reset as default dictionaries that then get oppose / favor for each sub-topic?\n",
    "    ## can set up \"GENERAL\" high-level and then \"IN-DEPTH\" if people want to get there, as well\n",
    "\n",
    "    #iterate through each line in the web page (text) until get to the votematch table\n",
    "    for line in text :\n",
    "        #once at votematch table, get to each section via \"a name\" divider\n",
    "        if \"a name\" in line or \"End Senatematch Responses\" in line:\n",
    "            #if not first time have hit \"a name\" section, reset, and add lines and points to dictionaries\n",
    "            if counter > 0 :\n",
    "                text_dict[counter] = lines\n",
    "                points_dict[counter] = points\n",
    "                topics_dict[counter] = (topic, category)\n",
    "                if \"End Senatematch Responses\" in line :\n",
    "                    #end of section, break for loop\n",
    "                    break\n",
    "\n",
    "            counter += 1\n",
    "            soup = BeautifulSoup(line, \"lxml\")\n",
    "            question_num = counter\n",
    "            lines = []\n",
    "\n",
    "        #if have hit \"a name\" section, then get number of points associated for that section\n",
    "        if counter > 0 :\n",
    "            lines.append(line)\n",
    "            if \"points on\" in line :\n",
    "                #grab number of points\n",
    "                points = line[line.find(\"(\")+1:line.find(\"points\")-1]\n",
    "                points = int(points)\n",
    "                #grab social vs economic category\n",
    "                if \"Social\" in line :\n",
    "                    category = \"Social\"\n",
    "                else :\n",
    "                    category = \"Economic\"\n",
    "            if \"</b> topic\" in line :\n",
    "                #grab topic mapping (only really necessary once...)\n",
    "                topic = line[line.find(\"'>\")+2:line.find(\"</a\")]\n",
    "\n",
    "\n",
    "    print \"POINTS DICT:\"\n",
    "    for i in points_dict.iteritems() :\n",
    "        print i\n",
    "\n",
    "    print\n",
    "    print \"TEXT DICT SECTION BREAKOUT\"\n",
    "    for i in text_dict.iteritems() :\n",
    "        print i\n",
    "        print \n",
    "\n",
    "    ### CREATE QUESTION -> TOPIC MAPPING FOR EACH GENERAL TOPIC ###\n",
    "    topics_list = []\n",
    "    for question, (topic, category) in topics_dict.iteritems() :\n",
    "        topics_list.append((question, topic, category))\n",
    "    print \"TOPICS LIST:\"\n",
    "    print topics_list\n",
    "\n",
    "\n",
    "    ### CREATE QUESTION -> POINTS MAPPING FOR EACH GENERAL TOPIC ###\n",
    "    points_list = []\n",
    "    for question, points in points_dict.iteritems() :\n",
    "        points_list.append((question, points))\n",
    "    print \"POINTS LIST:\"\n",
    "    print points_list\n",
    "\n",
    "\n",
    "    ### CONVERT KEY LISTS TO ARRAYS ###\n",
    "    ### NEEDED???\n",
    "    import numpy as np\n",
    "    points_array = np.array(points_list)\n",
    "    print \"POINTS ARRAY:\"\n",
    "    print points_array[0:5]\n",
    "\n",
    "    import numpy as np\n",
    "    topics_array = np.array(topics_list)\n",
    "    print \"TOPICS ARRAY:\"\n",
    "    print topics_array[0:5]\n",
    "\n",
    "\n",
    "    ### GET TOTAL POINTS FOR SOCIAL, ECONOMIC ###\n",
    "\n",
    "    #map category (Social, Economic) to points_list_category\n",
    "    #may want to clean up with a standard \"map\"\n",
    "    points_category_list = []\n",
    "    social = 0\n",
    "    economic = 0\n",
    "    for (question, points) in points_list :\n",
    "        category = topics_dict[question][1]\n",
    "        points_category_list.append((question, points, category))\n",
    "        #add points to relevant category\n",
    "        if category == \"Social\" :\n",
    "            social += points\n",
    "        else :\n",
    "            economic += points\n",
    "    print \"POINTS LIST WITH CATEGORY:\"\n",
    "    print points_category_list\n",
    "    print \n",
    "    print \"SOCIAL POINTS:\"\n",
    "    print social\n",
    "    print \"ECONOMIC POINTS:\"\n",
    "    print economic\n",
    "\n",
    "    ### CREATE QUESTION -> TEXT MAPPING FOR EACH GENERAL TOPIC ###\n",
    "    for question, text in text_dict.iteritems() :\n",
    "        start_tracking = False\n",
    "        for line in text :\n",
    "            print line,\n",
    "            #text and questions we're looking for are only post-<td>... or do we need to pick the single in favor / oppose before? \n",
    "            if \"<td>\" in line :\n",
    "                start_tracking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
