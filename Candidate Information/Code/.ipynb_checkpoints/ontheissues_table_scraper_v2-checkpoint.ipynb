{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup\n",
      "  Downloading BeautifulSoup-3.2.1.tar.gz\n",
      "Building wheels for collected packages: BeautifulSoup\n",
      "  Running setup.py bdist_wheel for BeautifulSoup: started\n",
      "  Running setup.py bdist_wheel for BeautifulSoup: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\kowen\\AppData\\Local\\pip\\Cache\\wheels\\5e\\be\\6d\\ed01d5d434a821557b674c9da976f60b1b93d9009447eb9d16\n",
      "Successfully built BeautifulSoup\n",
      "Installing collected packages: BeautifulSoup\n",
      "Successfully installed BeautifulSoup-3.2.1\n",
      "Requirement already satisfied: Requests in c:\\users\\kowen\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup\n",
    "!pip install Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial link: web scraping: http://first-web-scraper.readthedocs.io/en/latest/#act-3-web-scraping\n",
    "See notes at this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'AKERS', u'BRANDEN', u'MICHAEL', u'M', u'W', u'32', u'MOKANE', u'MO', u'Details'], [u'ALLEN', u'WILLIAM', u'LAMAR', u'M', u'B', u'55', u'COLUMBIA', u'MO', u'Details'], [u'ANDERSON', u'RICHARD', u'KENNETH', u'M', u'B', u'24', u'COLUMBIA', u'MO', u'Details'], [u'AVALOS-AVALOS', u'JOSE', u'', u'M', u'H', u'19', u'ST.ANN', u'MO', u'Details'], [u'BARNEY', u'FELSON', u'DEVONE', u'M', u'B', u'40', u'COLUMBIA', u'MO', u'Details'], [u'BARRON', u'DUSTYN', u'MICHAEL', u'M', u'W', u'20', u'HARTSBURG', u'MO', u'Details'], [u'BLAIR', u'RYAN', u'WADE', u'M', u'W', u'26', u'MEXICO', u'MO', u'Details'], [u'BLOCKER', u'TRAMELL', u'LAMARR', u'M', u'B', u'39', u'COLUMBIA', u'MO', u'Details'], [u'BOLEYN', u'TRENTON', u'DEAN', u'M', u'W', u'36', u'COLUMBIA', u'MO', u'Details'], [u'BONAPARTE', u'NATHANIEL', u'LEROY', u'M', u'B', u'38', u'COLUMBIA', u'MO', u'Details'], [u'BOSS', u'JAFARI', u'RASHIDI', u'M', u'B', u'34', u'COLUMBIA', u'MO', u'Details'], [u'BRADEN', u'WILLIAM', u'CHRISTOPHER', u'M', u'W', u'17', u'HALLSVILLE', u'MO', u'Details'], [u'BRADSHAW', u'JOSEPH', u'LEE', u'M', u'W', u'41', u'COLUMBIA', u'MO', u'Details'], [u'BRASHIER', u'APRIL', u'RAE', u'F', u'W', u'43', u'COLUMBIA', u'MO', u'Details'], [u'BRILL', u'BRANDON', u'NICHOLAS', u'M', u'W', u'26', u'CLARK', u'MO', u'Details'], [u'BROWN', u'CLAUDE', u'DAVID', u'M', u'W', u'52', u'COLUMBIA', u'MO', u'Details'], [u'BROWN', u'THYRUS', u'MONTEZ', u'M', u'B', u'48', u'COLUMBIA', u'MO', u'Details'], [u'BRUNS', u'MATTHEW', u'ELLIOT', u'M', u'W', u'21', u'KIRKWOOD', u'MO', u'Details'], [u'BURRIS', u'GEOFFREY', u'DARL', u'M', u'W', u'28', u'COLUMBIA', u'MO', u'Details'], [u'BUSH', u'FRANK', u'LADON', u'M', u'B', u'39', u'COLUMBIA', u'MO', u'Details'], [u'CALVERT', u'JOHN', u'RAY', u'M', u'W', u'35', u'COLUMBIA', u'MO', u'Details'], [u'CALVIN', u'KAYLA', u'ANN', u'F', u'W', u'30', u'COLUMBIA', u'MO', u'Details'], [u'CANFIELD', u'NATHAN', u'EDWARD', u'M', u'W', u'35', u'COLUMBIA', u'MO', u'Details'], [u'CAREAGA', u'JOSEPH', u'EUGENE', u'M', u'W', u'47', u'MADISON', u'MO', u'Details'], [u'CARTER', u'DARIAN', u'MAURICE', u'M', u'B', u'24', u'COLUMBIA', u'MO', u'Details'], [u'CARTER', u'DARREN', u'DEVON', u'M', u'B', u'22', u'COLUMBIA', u'MO', u'Details'], [u'CARTER', u'DEMARCO', u'RAYDELL', u'M', u'B', u'22', u'COLUMBIA', u'MO', u'Details'], [u'CASE', u'JACOB', u'MICHAEL', u'M', u'W', u'24', u'COLUMBIA', u'MO', u'Details'], [u'CERVANTES', u'FERNANDO', u'GUILLEN', u'M', u'H', u'24', u'COLUMBIA', u'MO', u'Details'], [u'CLARK', u'DEVIN', u'J', u'M', u'W', u'42', u'COLUMBIA', u'MO', u'Details'], [u'COATES', u'JOSHUA', u'EDWARD', u'M', u'W', u'36', u'ROCHEPORT', u'MO', u'Details'], [u'COCO', u'DREW', u'PATRICK', u'M', u'W', u'37', u'COLUMBIA', u'MO', u'Details'], [u'COLEMAN', u'JAMES', u'MICHAEL', u'M', u'W', u'48', u'COLUMBIA', u'MO', u'Details'], [u'COLEMAN', u'WESTLY', u'SHAWN', u'M', u'W', u'43', u'COLUMBIA', u'MO', u'Details'], [u'COLLINS', u'DENNIS', u'CARL', u'M', u'B', u'33', u'COLUMBIA', u'MO', u'Details'], [u'COOK', u'RICKY', u'BLAINE', u'M', u'W', u'46', u'COLUMBIA', u'MO', u'Details'], [u'COPENHAVER', u'TONY', u'EUGENE', u'M', u'W', u'49', u'COLUMBIA', u'MO', u'Details'], [u'CORNELL', u'GERALD', u'HOLLIS', u'M', u'W', u'62', u'MURRAY', u'UT', u'Details'], [u'CROWLEY', u'ROBERT', u'JASON', u'M', u'W', u'33', u'COLUMBIA', u'MO', u'Details'], [u'CROY', u'MARANDA', u'LEE', u'F', u'W', u'31', u'COLUMBIA', u'MO', u'Details'], [u'DANIELSSON', u'JESSE', u'ROBERTO', u'M', u'W', u'24', u'LAYTON', u'UT', u'Details'], [u'DAVIS', u'KAYLA', u'REBECCA', u'F', u'W', u'27', u'WOOLRIDGE', u'MO', u'Details'], [u'DAWSON', u'VINCENT', u'GESHAI', u'M', u'B', u'20', u'COLUMBIA', u'MO', u'Details'], [u'DAY', u'MICHAEL', u'BRIAN', u'M', u'W', u'46', u'COLUMBIA', u'MO', u'Details'], [u'DICKERSON', u'CHRISTOPHER', u'KEWESI', u'M', u'B', u'19', u'NEW ORLEANS', u'LA', u'Details'], [u'DIXON', u'JAY', u'WAYNE', u'M', u'B', u'36', u'COLUMBIA', u'MO', u'Details'], [u'DOWNS', u'WILLIAM', u'CHARLES', u'M', u'W', u'42', u'JEFFERSON CITY', u'MO', u'Details'], [u'DUGAN', u'KAYLA', u'NICOLE', u'F', u'W', u'29', u'CALIFORNIA', u'MO', u'Details'], [u'DUNCAN', u'DUSTY', u'LANE', u'M', u'W', u'22', u'COLUMBIA', u'MO', u'Details'], []]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "url = 'http://www.showmeboone.com/sheriff/JailResidents/JailResidents.asp'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "table = soup.find('tbody', attrs={'class': 'stripe'})\n",
    "\n",
    "list_of_rows = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    list_of_cells = []\n",
    "    for cell in row.findAll('td'):\n",
    "        text = cell.text.replace('&nbsp;', '')\n",
    "        list_of_cells.append(text)\n",
    "    list_of_rows.append(list_of_cells)\n",
    "\n",
    "print list_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "url = 'http://www.showmeboone.com/sheriff/JailResidents/JailResidents.asp'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "table = soup.find('tbody', attrs={'class': 'stripe'})\n",
    "\n",
    "list_of_rows = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    list_of_cells = []\n",
    "    for cell in row.findAll('td'):\n",
    "        text = cell.text.replace('&nbsp;', '')\n",
    "        list_of_cells.append(text)\n",
    "    list_of_rows.append(list_of_cells)\n",
    "\n",
    "outfile = open(\"./inmates.csv\", \"wb\")\n",
    "writer = csv.writer(outfile)\n",
    "writer.writerow([\"Last\", \"First\", \"Middle\", \"Gender\", \"Race\", \"Age\", \"City\", \"State\"])\n",
    "writer.writerows(list_of_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    " - Iterate through, whenever at anchor, store anchor name\n",
    " - If is for a target table, load table into memory\n",
    " - Once hit next anchor, done\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d43823ee3257>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mlist_of_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mlist_of_cells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "url = 'http://www.ontheissues.org/Cory_Booker.htm'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "table = soup.find('tbody', attrs={'a id': 'Health_Care'})\n",
    "\n",
    "### NEED TO BE ABLE TO FIND A WAY TO TAKE TABLES IN BETWEEN A_IDs\n",
    "anchor = soup.findAll('a id')\n",
    "\n",
    "print anchor\n",
    "\n",
    "list_of_rows = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    list_of_cells = []\n",
    "    for cell in row.findAll('td'):\n",
    "        text = cell.text.replace('&nbsp;', '')\n",
    "        list_of_cells.append(text)\n",
    "    list_of_rows.append(list_of_cells)\n",
    "\n",
    "print list_of_rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "url = 'http://www.ontheissues.org/Cory_Booker.htm'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "table = soup.find('tbody')\n",
    "print table\n",
    "# table = soup.find('tbody', attrs={'a id': 'Health_Care'})\n",
    "\n",
    "### NEED TO BE ABLE TO FIND A WAY TO TAKE TABLES IN BETWEEN A_IDs\n",
    "anchor = soup.find('a',attrs={'id':\"Civil_Rights\"})\n",
    "comment = soup.find('Issue Budget & Economy')\n",
    "\n",
    "print anchor\n",
    "print comment\n",
    "\n",
    "# list_of_rows = []\n",
    "# for row in table.findAll('tr')[1:]:\n",
    "#     list_of_cells = []\n",
    "#     for cell in row.findAll('td'):\n",
    "#         text = cell.text.replace('&nbsp;', '')\n",
    "#         list_of_cells.append(text)\n",
    "#     list_of_rows.append(list_of_cells)\n",
    "\n",
    "# print list_of_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in comment\n",
      " in comment\n",
      " also in comment\n",
      " also in comment\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/34673851/extracting-text-between-html-comments-with-beautifulsoup\n",
    "### GET TEXT BETWEEN COMMENTS\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://www.ontheissues.org/Cory_Booker.htm'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "#way 1\n",
    "for comment in soup.findAll(text=lambda text:isinstance(text, Comment)):\n",
    "    if comment in [' Issue Civil Rights completed ', ' Issue Corporations completed ']:\n",
    "        print comment.next_element.strip(), \"in comment\"\n",
    "\n",
    "#way 2\n",
    "comments_to_search_for = {' Issue Civil Rights completed ', ' Issue Corporations completed '}\n",
    "for comment in soup.find_all(text=lambda text: isinstance(text, Comment) and text in comments_to_search_for):\n",
    "    print(comment.next_element.strip()), \"also in comment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_to_search_for = {' Issue Civil Rights completed ', ' Issue Corporations completed '}\n",
    "for comment in soup.find_all(text=lambda text: isinstance(text, Comment) and text in comments_to_search_for):\n",
    "    print(comment.next_element.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
